{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":105399,"databundleVersionId":12733338,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ✈️ FlightRank 2025: Aeroclub RecSys Cup¶\n\n**Personalized Flight Recommendations for Business Travelers**\n\n\n","metadata":{}},{"cell_type":"markdown","source":"![FlightRank Header](https://raw.githubusercontent.com/Ishita95-harvad/flightrank-2025-aeroclub-recsys-cup/main/header.png)","metadata":{}},{"cell_type":"markdown","source":"**A data-driven journey into flight recommendation systems. Presented by Aeroclub for the RecSys Cup 2025, this visual blends aviation with algorithmic precision—featuring a stylized aircraft built from numerical data points, set against a sleek grid backdrop. Innovation takes flight**","metadata":{}},{"cell_type":"markdown","source":"--------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"**Step 1: Initial Setup and Data Loading**","metadata":{}},{"cell_type":"code","source":"# Import essential libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport os\n\n# Set up notebook display options\npd.set_option('display.max_columns', 200)\nplt.style.use('ggplot')\n\n# Load the data\ntrain = pd.read_parquet('/kaggle/input/flight-selection-prediction/train.parquet')\ntest = pd.read_parquet('/kaggle/input/flight-selection-prediction/test.parquet')\nsample_sub = pd.read_parquet('/kaggle/input/flight-selection-prediction/sample_submission.parquet')\n\n# Quick checks\nprint(f\"Train shape: {train.shape}\")\nprint(f\"Test shape: {test.shape}\")\nprint(f\"Sample submission shape: {sample_sub.shape}\")\n\n# Display first few rows\ntrain.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 2: Exploratory Data Analysis (EDA)**","metadata":{}},{"cell_type":"code","source":"# Basic statistics\ntrain.describe(include='all')\n\n# Target variable distribution\nplt.figure(figsize=(8,5))\nsns.countplot(x='selected', data=train)\nplt.title('Target Variable Distribution')\nplt.show()\n\n# Check number of options per search session\noptions_per_search = train.groupby('ranker_id').size().value_counts()\nplt.figure(figsize=(10,5))\noptions_per_search.plot(kind='bar')\nplt.title('Number of Flight Options per Search Session')\nplt.xlabel('Number of Options')\nplt.ylabel('Count')\nplt.show()\n\n# Analyze numerical features\nnum_cols = train.select_dtypes(include=np.number).columns.tolist()\nnum_cols.remove('selected')  # Remove target\n\nplt.figure(figsize=(15, 20))\nfor i, col in enumerate(num_cols[:15]):  # First 15 numerical columns\n    plt.subplot(5, 3, i+1)\n    sns.histplot(train[col], kde=True, bins=30)\n    plt.title(col)\nplt.tight_layout()\nplt.show()\n\n# Analyze categorical features\ncat_cols = ['sex', 'nationality', 'frequentFlyer', 'isVip', 'bySelf', 'isAccess3D', 'corporateTariffCode']\n\nplt.figure(figsize=(15, 15))\nfor i, col in enumerate(cat_cols):\n    plt.subplot(3, 3, i+1)\n    sns.countplot(x=col, data=train)\n    plt.title(col)\n    plt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 3: Feature Engineering**","metadata":{}},{"cell_type":"code","source":"# Feature engineering function\ndef create_features(df):\n    # Copy dataframe to avoid SettingWithCopyWarning\n    df = df.copy()\n    \n    # Price features\n    df['price_per_km'] = df['totalPrice'] / (df['legs0_duration'] + df.get('legs1_duration', 0))\n    \n    # Time features\n    df['departure_hour'] = pd.to_datetime(df['legs0_departureAt']).dt.hour\n    df['is_weekend'] = pd.to_datetime(df['legs0_departureAt']).dt.weekday >= 5\n    \n    # Duration features\n    df['total_duration'] = df['legs0_duration'] + df.get('legs1_duration', 0)\n    df['duration_per_km'] = df['total_duration'] / df['totalPrice']\n    \n    # Cabin class features (assuming highest class determines overall class)\n    df['max_cabin_class'] = df[[f'legs{i}_segments0_cabinClass' for i in [0,1] if f'legs{i}_segments0_cabinClass' in df.columns]].max(axis=1)\n    \n    # Baggage features\n    for leg in [0,1]:\n        seg_col = f'legs{leg}_segments0_baggageAllowance_quantity'\n        if seg_col in df.columns:\n            df[f'legs{leg}_baggage'] = df[seg_col]\n    \n    # Cancellation penalty features\n    if 'miniRules0_monetaryAmount' in df.columns:\n        df['total_cancellation_penalty'] = df['miniRules0_monetaryAmount'] + (df['totalPrice'] * df['miniRules0_percentage'] / 100)\n    \n    return df\n\n# Apply feature engineering\ntrain_fe = create_features(train)\ntest_fe = create_features(test)\n\n# Display new features\ntrain_fe[['price_per_km', 'departure_hour', 'is_weekend', 'total_duration', 'max_cabin_class']].head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T19:01:58.809564Z","iopub.execute_input":"2025-08-13T19:01:58.809851Z","iopub.status.idle":"2025-08-13T19:01:58.825388Z","shell.execute_reply.started":"2025-08-13T19:01:58.809829Z","shell.execute_reply":"2025-08-13T19:01:58.823905Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1205148571.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Apply feature engineering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtrain_fe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mtest_fe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"],"ename":"NameError","evalue":"name 'train' is not defined","output_type":"error"}],"execution_count":4},{"cell_type":"markdown","source":"**Step 4: Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import GroupKFold\n\n# Identify columns to use\nuseful_cols = [\n    # User features\n    'sex', 'nationality', 'frequentFlyer', 'isVip', 'bySelf',\n    \n    # Flight features\n    'totalPrice', 'taxes', 'legs0_duration', 'legs1_duration',\n    'legs0_segments0_cabinClass', 'legs1_segments0_cabinClass',\n    'legs0_segments0_baggageAllowance_quantity', 'legs1_segments0_baggageAllowance_quantity',\n    \n    # Engineered features\n    'price_per_km', 'departure_hour', 'is_weekend', 'total_duration', 'max_cabin_class'\n]\n\n# Filter columns (keep only those present in both train and test)\navailable_cols = [col for col in useful_cols if col in train_fe.columns and col in test_fe.columns]\ntarget = 'selected'\n\n# Prepare data\nX = train_fe[available_cols]\ny = train_fe[target]\ngroups = train_fe['ranker_id']\nX_test = test_fe[available_cols]\n\n# Handle categorical variables\ncat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\nfor col in cat_cols:\n    le = LabelEncoder()\n    # Combine train and test to handle all categories\n    combined = pd.concat([X[col], X_test[col]], axis=0)\n    le.fit(combined)\n    X[col] = le.transform(X[col])\n    X_test[col] = le.transform(X_test[col])\n\n# Scale numerical features\nnum_cols = [col for col in available_cols if col not in cat_cols]\nscaler = StandardScaler()\nX[num_cols] = scaler.fit_transform(X[num_cols])\nX_test[num_cols] = scaler.transform(X_test[num_cols])\n\n# Display processed data\nX.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T19:01:58.885383Z","iopub.execute_input":"2025-08-13T19:01:58.885720Z","iopub.status.idle":"2025-08-13T19:01:59.260394Z","shell.execute_reply.started":"2025-08-13T19:01:58.885696Z","shell.execute_reply":"2025-08-13T19:01:59.258652Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/582768961.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Filter columns (keep only those present in both train and test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mavailable_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0museful_cols\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_fe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_fe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'selected'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/582768961.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Filter columns (keep only those present in both train and test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mavailable_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0museful_cols\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_fe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_fe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'selected'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_fe' is not defined"],"ename":"NameError","evalue":"name 'train_fe' is not defined","output_type":"error"}],"execution_count":5},{"cell_type":"markdown","source":"**Step 5: Model Training with Cross-Validation**","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.metrics import ndcg_score\n\n# Custom evaluation metric for ranking\ndef ndcg_scorer(estimator, X, y):\n    # Get probabilities\n    y_pred = estimator.predict_proba(X)[:, 1]\n    \n    # Create groups for NDCG calculation\n    unique_groups = groups.loc[X.index].unique()\n    ndcg_scores = []\n    \n    for group in unique_groups:\n        group_mask = (groups.loc[X.index] == group)\n        y_true_group = y[group_mask]\n        y_pred_group = y_pred[group_mask]\n        \n        # Reshape for ndcg_score\n        y_true_group = y_true_group.values.reshape(1, -1)\n        y_pred_group = y_pred_group.reshape(1, -1)\n        \n        ndcg_scores.append(ndcg_score(y_true_group, y_pred_group))\n    \n    return np.mean(ndcg_scores)\n\n# Set up LightGBM parameters\nparams = {\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'boosting_type': 'gbdt',\n    'learning_rate': 0.05,\n    'num_leaves': 31,\n    'min_child_samples': 20,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 1,\n    'verbose': -1,\n    'seed': 42\n}\n\n# GroupKFold cross-validation\nn_folds = 5\ngkf = GroupKFold(n_splits=n_folds)\noof_preds = np.zeros(len(X))\nmodels = []\n\nfor fold, (train_idx, valid_idx) in enumerate(gkf.split(X, y, groups=groups)):\n    print(f\"\\nFold {fold + 1}\")\n    \n    # Split data\n    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n    \n    # LightGBM dataset\n    train_set = lgb.Dataset(X_train, y_train)\n    valid_set = lgb.Dataset(X_valid, y_valid, reference=train_set)\n    \n    # Train model\n    model = lgb.train(\n        params,\n        train_set,\n        num_boost_round=1000,\n        valid_sets=[valid_set],\n        early_stopping_rounds=50,\n        verbose_eval=100\n    )\n    \n    # Store model and predictions\n    oof_preds[valid_idx] = model.predict(X_valid)\n    models.append(model)\n    \n    # Calculate NDCG for this fold\n    fold_ndcg = ndcg_scorer(model, X_valid, y_valid)\n    print(f\"Fold {fold + 1} NDCG: {fold_ndcg:.4f}\")\n\n# Overall OOF NDCG\nunique_groups = groups.unique()\nndcg_scores = []\n\nfor group in unique_groups:\n    group_mask = (groups == group)\n    y_true_group = y[group_mask]\n    y_pred_group = oof_preds[group_mask]\n    \n    y_true_group = y_true_group.values.reshape(1, -1)\n    y_pred_group = y_pred_group.reshape(1, -1)\n    \n    ndcg_scores.append(ndcg_score(y_true_group, y_pred_group))\n\nprint(f\"\\nOverall OOF NDCG: {np.mean(ndcg_scores):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T19:01:59.260896Z","iopub.status.idle":"2025-08-13T19:01:59.261170Z","shell.execute_reply.started":"2025-08-13T19:01:59.261046Z","shell.execute_reply":"2025-08-13T19:01:59.261056Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 6: Feature Importance Analysis**","metadata":{}},{"cell_type":"code","source":"# Plot feature importance\nplt.figure(figsize=(12, 8))\nlgb.plot_importance(models[0], max_num_features=20, importance_type='gain')\nplt.title('Feature Importance (Gain)')\nplt.show()\n\n# Get feature importance dataframe\nimportance_df = pd.DataFrame({\n    'feature': models[0].feature_name(),\n    'importance': models[0].feature_importance(importance_type='gain')\n}).sort_values('importance', ascending=False)\n\nimportance_df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T19:01:59.262300Z","iopub.status.idle":"2025-08-13T19:01:59.262719Z","shell.execute_reply.started":"2025-08-13T19:01:59.262531Z","shell.execute_reply":"2025-08-13T19:01:59.262549Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 7: Generate Predictions and Submission File**","metadata":{}},{"cell_type":"code","source":"# Generate test predictions (average across folds)\ntest_preds = np.zeros(len(X_test))\nfor model in models:\n    test_preds += model.predict(X_test) / len(models)\n\n# Create submission dataframe\nsubmission = test[['Id', 'ranker_id']].copy()\n\n# Assign ranks within each search session (ranker_id)\nsubmission['selected'] = 0  # Initialize\n\nfor ranker_id in tqdm(submission['ranker_id'].unique()):\n    mask = submission['ranker_id'] == ranker_id\n    # Rank predictions (1 is best, higher numbers worse)\n    submission.loc[mask, 'selected'] = (-test_preds[mask]).argsort().argsort() + 1\n\n# Verify ranks are valid\nfor ranker_id in submission['ranker_id'].unique()[:5]:  # Check first 5 groups\n    group_ranks = submission[submission['ranker_id'] == ranker_id]['selected']\n    print(f\"Group {ranker_id} ranks: {sorted(group_ranks.values)}\")\n\n# Save submission\nsubmission.to_csv('submission.csv', index=False)\n\n# Display sample submission\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T19:01:59.264176Z","iopub.status.idle":"2025-08-13T19:01:59.264567Z","shell.execute_reply.started":"2025-08-13T19:01:59.264362Z","shell.execute_reply":"2025-08-13T19:01:59.264378Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 8: Advanced Techniques (Optional)**","metadata":{}},{"cell_type":"code","source":"# Optional: Hyperparameter Optimization\nimport optuna\n\ndef objective(trial):\n    params = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'boosting_type': 'gbdt',\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n        'verbose': -1\n    }\n    \n    # Use first fold for optimization\n    train_idx, valid_idx = next(gkf.split(X, y, groups=groups))\n    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n    \n    train_set = lgb.Dataset(X_train, y_train)\n    valid_set = lgb.Dataset(X_valid, y_valid)\n    \n    model = lgb.train(\n        params,\n        train_set,\n        num_boost_round=500,\n        valid_sets=[valid_set],\n        early_stopping_rounds=50,\n        verbose_eval=False\n    )\n    \n    return ndcg_scorer(model, X_valid, y_valid)\n\n# Run optimization (commented out as it takes time)\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=20)\n# print(\"Best trial:\")\n# trial = study.best_trial\n# print(f\"NDCG: {trial.value:.4f}\")\n# print(f\"Best params: {trial.params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T19:01:59.265500Z","iopub.status.idle":"2025-08-13T19:01:59.265779Z","shell.execute_reply.started":"2025-08-13T19:01:59.265663Z","shell.execute_reply":"2025-08-13T19:01:59.265673Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**JSON files**","metadata":{}},{"cell_type":"code","source":"# Example of processing JSONs in chunks (if needed)\nimport json\nimport tarfile\n\ndef process_json_chunks(tar_path, chunk_size=1000):\n    features = []\n    with tarfile.open(tar_path, 'r:gz') as tar:\n        members = tar.getmembers()\n        for i in range(0, len(members), chunk_size):\n            chunk = members[i:i+chunk_size]\n            for member in chunk:\n                f = tar.extractfile(member)\n                data = json.load(f)\n                # Extract features from JSON\n                # features.append(...)\n    return pd.DataFrame(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T19:01:59.267706Z","iopub.status.idle":"2025-08-13T19:01:59.268006Z","shell.execute_reply.started":"2025-08-13T19:01:59.267872Z","shell.execute_reply":"2025-08-13T19:01:59.267882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verified Notebook Setup","metadata":{}},{"cell_type":"code","source":"# Import all required libraries (verified)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport os\nimport lightgbm as lgb\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import ndcg_score\nimport json\nimport tarfile\n\n# Set display options\npd.set_option('display.max_columns', 200)\nplt.style.use('ggplot')\n\n# Verify input directory structure\ninput_dir = '/kaggle/input/aeroclub-recsys-2025'\nprint(\"Input directory contents:\")\nprint(os.listdir(input_dir))\n\n# Load data with verified paths\ntrain = pd.read_parquet(f'{input_dir}/train.parquet')\ntest = pd.read_parquet(f'{input_dir}/test.parquet')\nsample_sub = pd.read_parquet(f'{input_dir}/sample_submission.parquet')\n\n# Data verification\nprint(f\"\\nTrain shape: {train.shape}, columns: {train.columns.tolist()}\")\nprint(f\"Test shape: {test.shape}\")\nprint(f\"Sample submission shape: {sample_sub.shape}\")\nprint(\"\\nTrain target distribution:\")\nprint(train['selected'].value_counts(normalize=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T19:01:59.269274Z","iopub.status.idle":"2025-08-13T19:01:59.269956Z","shell.execute_reply.started":"2025-08-13T19:01:59.269783Z","shell.execute_reply":"2025-08-13T19:01:59.269800Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Data Exploration and Visualization**","metadata":{}},{"cell_type":"code","source":"# Visualize target distribution\nplt.figure(figsize=(10, 5))\ntrain['selected'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\nplt.title('Target Variable Distribution (0=Not Selected, 1=Selected)')\nplt.xticks(rotation=0)\nplt.show()\n\n# Analyze options per search session\noptions_per_search = train.groupby('ranker_id').size()\nplt.figure(figsize=(12, 6))\noptions_per_search.value_counts().sort_index().plot(kind='bar', color='teal')\nplt.title('Number of Flight Options per Search Session')\nplt.xlabel('Number of Options')\nplt.ylabel('Count of Search Sessions')\nplt.show()\n\n# Price analysis\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nsns.histplot(train['totalPrice'], bins=50, color='royalblue')\nplt.title('Distribution of Total Price')\nplt.subplot(1, 2, 2)\nsns.boxplot(x='selected', y='totalPrice', data=train)\nplt.title('Price Distribution by Selection Status')\nplt.tight_layout()\nplt.show()\n\n# Flight duration analysis\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nsns.histplot(train['legs0_duration'], bins=50, color='purple')\nplt.title('Outbound Flight Duration')\nplt.subplot(1, 2, 2)\nsns.boxplot(x='selected', y='legs0_duration', data=train)\nplt.title('Duration Distribution by Selection Status')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T19:01:59.270854Z","iopub.status.idle":"2025-08-13T19:01:59.271152Z","shell.execute_reply.started":"2025-08-13T19:01:59.270985Z","shell.execute_reply":"2025-08-13T19:01:59.270994Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Feature Engineering (Updated)**","metadata":{}},{"cell_type":"code","source":"def create_features(df):\n    df = df.copy()\n    \n    # Price features\n    df['price_to_taxes_ratio'] = df['totalPrice'] / (df['taxes'] + 1)\n    \n    # Time features\n    df['legs0_departure_hour'] = pd.to_datetime(df['legs0_departureAt']).dt.hour\n    df['legs0_departure_day'] = pd.to_datetime(df['legs0_departureAt']).dt.dayofweek\n    df['is_weekend'] = df['legs0_departure_day'] >= 5\n    \n    # Duration features\n    df['total_duration'] = df['legs0_duration'] + df.get('legs1_duration', 0)\n    df['duration_per_price'] = df['total_duration'] / (df['totalPrice'] + 1)\n    \n    # Cabin class features\n    cabin_cols = [f'legs{i}_segments{j}_cabinClass' \n                 for i in [0,1] for j in [0] \n                 if f'legs{i}_segments{j}_cabinClass' in df.columns]\n    if cabin_cols:\n        df['max_cabin_class'] = df[cabin_cols].max(axis=1)\n    \n    # Baggage features\n    baggage_cols = [f'legs{i}_segments0_baggageAllowance_quantity' \n                   for i in [0,1] \n                   if f'legs{i}_segments0_baggageAllowance_quantity' in df.columns]\n    if baggage_cols:\n        df['total_baggage'] = df[baggage_cols].sum(axis=1)\n    \n    # Cancellation features\n    if 'miniRules0_monetaryAmount' in df.columns:\n        df['total_cancellation_penalty'] = (\n            df['miniRules0_monetaryAmount'] + \n            (df['totalPrice'] * df['miniRules0_percentage'] / 100)\n    \n    return df\n\n# Apply feature engineering\ntrain_fe = create_features(train)\ntest_fe = create_features(test)\n\n# Display new features\nprint(\"Engineered features sample:\")\ndisplay(train_fe[[\n    'price_to_taxes_ratio', \n    'legs0_departure_hour',\n    'total_duration',\n    'max_cabin_class'\n]].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T19:01:59.289178Z","iopub.execute_input":"2025-08-13T19:01:59.289587Z","iopub.status.idle":"2025-08-13T19:01:59.302586Z","shell.execute_reply.started":"2025-08-13T19:01:59.289553Z","shell.execute_reply":"2025-08-13T19:01:59.301271Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_36/3859334657.py\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    df['total_cancellation_penalty'] = (\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"],"ename":"SyntaxError","evalue":"'(' was never closed (3859334657.py, line 32)","output_type":"error"}],"execution_count":6},{"cell_type":"markdown","source":"**Model Training with HitRate@3 Optimization**","metadata":{}},{"cell_type":"code","source":"# Prepare data for modeling\nuseful_cols = [\n    # Basic features\n    'totalPrice', 'taxes', 'legs0_duration', 'legs1_duration',\n    \n    # User features\n    'sex', 'nationality', 'frequentFlyer', 'isVip',\n    \n    # Cabin class\n    'legs0_segments0_cabinClass', 'legs1_segments0_cabinClass',\n    \n    # Engineered features\n    'price_to_taxes_ratio', 'legs0_departure_hour', \n    'total_duration', 'max_cabin_class', 'total_baggage'\n]\n\n# Filter available columns\navailable_cols = [col for col in useful_cols if col in train_fe.columns and col in test_fe.columns]\ntarget = 'selected'\n\nX = train_fe[available_cols]\ny = train_fe[target]\ngroups = train_fe['ranker_id']\nX_test = test_fe[available_cols]\n\n# Preprocessing\ncat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\nfor col in cat_cols:\n    le = LabelEncoder()\n    combined = pd.concat([X[col], X_test[col]], axis=0)\n    le.fit(combined)\n    X[col] = le.transform(X[col])\n    X_test[col] = le.transform(X_test[col])\n\nnum_cols = [col for col in available_cols if col not in cat_cols]\nscaler = StandardScaler()\nX[num_cols] = scaler.fit_transform(X[num_cols])\nX_test[num_cols] = scaler.transform(X_test[num_cols])\n\n# Custom HitRate@3 metric\ndef hit_rate_at_3(y_true, y_pred, groups):\n    results = []\n    for group in groups.unique():\n        group_mask = (groups == group)\n        y_true_group = y_true[group_mask]\n        y_pred_group = y_pred[group_mask]\n        \n        # Get top 3 predictions\n        top3 = y_pred_group.argsort()[-3:][::-1]\n        hit = y_true_group.iloc[top3].sum() > 0\n        results.append(hit)\n    \n    return np.mean(results)\n\n# LightGBM parameters optimized for ranking\nparams = {\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'boosting_type': 'gbdt',\n    'learning_rate': 0.05,\n    'num_leaves': 63,\n    'min_child_samples': 30,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 1,\n    'lambda_l1': 0.1,\n    'lambda_l2': 0.1,\n    'verbose': -1,\n    'seed': 42\n}\n\n# GroupKFold cross-validation\nn_folds = 5\ngkf = GroupKFold(n_splits=n_folds)\noof_preds = np.zeros(len(X))\nmodels = []\nhit_rates = []\n\nfor fold, (train_idx, valid_idx) in enumerate(gkf.split(X, y, groups=groups)):\n    print(f\"\\nFold {fold + 1}\")\n    \n    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n    groups_valid = groups.iloc[valid_idx]\n    \n    train_set = lgb.Dataset(X_train, y_train)\n    valid_set = lgb.Dataset(X_valid, y_valid)\n    \n    model = lgb.train(\n        params,\n        train_set,\n        num_boost_round=1000,\n        valid_sets=[valid_set],\n        early_stopping_rounds=50,\n        verbose_eval=100\n    )\n    \n    oof_preds[valid_idx] = model.predict(X_valid)\n    models.append(model)\n    \n    # Calculate HitRate@3 for this fold\n    fold_hit_rate = hit_rate_at_3(y_valid, oof_preds[valid_idx], groups_valid)\n    hit_rates.append(fold_hit_rate)\n    print(f\"Fold {fold + 1} HitRate@3: {fold_hit_rate:.4f}\")\n\n# Overall OOF HitRate@3\noverall_hit_rate = hit_rate_at_3(y, oof_preds, groups)\nprint(f\"\\nOverall OOF HitRate@3: {overall_hit_rate:.4f}\")\nprint(f\"Average fold HitRate@3: {np.mean(hit_rates):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T19:01:59.320989Z","iopub.execute_input":"2025-08-13T19:01:59.321936Z","iopub.status.idle":"2025-08-13T19:01:59.359385Z","shell.execute_reply.started":"2025-08-13T19:01:59.321879Z","shell.execute_reply":"2025-08-13T19:01:59.357570Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/4069860235.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Filter available columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mavailable_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0museful_cols\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_fe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_fe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'selected'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/4069860235.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Filter available columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mavailable_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0museful_cols\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_fe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_fe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'selected'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_fe' is not defined"],"ename":"NameError","evalue":"name 'train_fe' is not defined","output_type":"error"}],"execution_count":7},{"cell_type":"markdown","source":"**Generate Submission File**","metadata":{}},{"cell_type":"code","source":"# Generate test predictions\ntest_preds = np.zeros(len(X_test))\nfor model in models:\n    test_preds += model.predict(X_test) / len(models)\n\n# Create submission with ranks\nsubmission = test[['Id', 'ranker_id']].copy()\n\n# Assign ranks within each search session (1 = best)\nfor ranker_id in tqdm(submission['ranker_id'].unique()):\n    mask = submission['ranker_id'] == ranker_id\n    submission.loc[mask, 'selected'] = (-test_preds[mask]).argsort().argsort() + 1\n\n# Verify submission format\nprint(\"\\nSubmission verification:\")\nprint(f\"Unique ranks in first group: {submission[submission['ranker_id'] == submission['ranker_id'].iloc[0]]['selected'].unique()}\")\n\n# Save submission\nsubmission.to_csv('submission.csv', index=False)\nprint(\"\\nSubmission file created: submission.csv\")\n\n# Display sample\nprint(\"\\nSample submission:\")\ndisplay(submission.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Feature Importance Analysis**","metadata":{}},{"cell_type":"code","source":"# Plot feature importance\nplt.figure(figsize=(12, 8))\nlgb.plot_importance(models[0], max_num_features=20, importance_type='gain')\nplt.title('Feature Importance (Gain)')\nplt.show()\n\n# Create importance dataframe\nimportance_df = pd.DataFrame({\n    'feature': models[0].feature_name(),\n    'importance': models[0].feature_importance(importance_type='gain')\n}).sort_values('importance', ascending=False)\n\nprint(\"Top 10 important features:\")\ndisplay(importance_df.head(10))\n\n# Plot top features\nplt.figure(figsize=(12, 6))\nsns.barplot(x='importance', y='feature', data=importance_df.head(10), palette='viridis')\nplt.title('Top 10 Features by Importance')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**JSON Data Processing (Optional)**","metadata":{}},{"cell_type":"code","source":"# Optional: Process JSON raw data\ndef extract_json_features(json_path):\n    \"\"\"Example function to process JSON files\"\"\"\n    features = []\n    with tarfile.open(json_path, 'r:gz') as tar:\n        for member in tqdm(tar.getmembers()[:1000]):  # Process first 1000 files as example\n            f = tar.extractfile(member)\n            data = json.load(f)\n            \n            # Extract relevant features from JSON\n            feature = {\n                'ranker_id': member.name.split('.')[0],\n                # Add other features from JSON structure\n            }\n            features.append(feature)\n    return pd.DataFrame(features)\n\n# Uncomment to process JSONs (requires significant memory)\n# json_path = f'{input_dir}/jsons_raw.tar.kaggle'\n# json_features = extract_json_features(json_path)\n# print(f\"Extracted {len(json_features)} JSON features\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**Complete Workflow**","metadata":{}},{"cell_type":"markdown","source":"--------------------","metadata":{}},{"cell_type":"markdown","source":"**1. Understanding HitRate@3 Implementation**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\ndef calculate_hitrate_at_3(submission_df, test_df):\n    \"\"\"\n    Calculates HitRate@3 metric for the competition\n    \n    Args:\n        submission_df: Your ranked predictions (must contain 'ranker_id' and 'selected' columns)\n        test_df: Original test data with ground truth ('selected' column)\n    \n    Returns:\n        hitrate_at_3: The competition metric score\n    \"\"\"\n    # Merge submission with ground truth\n    merged = submission_df.merge(test_df[['Id', 'ranker_id', 'selected']], \n                               on=['Id', 'ranker_id'], \n                               suffixes=('_pred', '_true'))\n    \n    # Filter groups with >10 options (as per competition rules)\n    group_sizes = merged.groupby('ranker_id').size()\n    valid_groups = group_sizes[group_sizes > 10].index\n    filtered = merged[merged['ranker_id'].isin(valid_groups)]\n    \n    # Calculate HitRate@3 for each group\n    hitrates = []\n    for ranker_id, group in filtered.groupby('ranker_id'):\n        # Get the true selected flight\n        true_selected = group[group['selected_true'] == 1].iloc[0]\n        \n        # Check if it's in top 3 predictions\n        group['rank'] = group['selected_pred']  # Your predicted ranks\n        top_3_flights = group.nsmallest(3, 'rank')['Id'].values\n        hit = true_selected['Id'] in top_3_flights\n        hitrates.append(hit)\n    \n    # Return overall hitrate\n    return np.mean(hitrates) if hitrates else 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Model Training with HitRate@3 Optimization**","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMRanker\nfrom sklearn.model_selection import GroupKFold\n\ndef train_ranker(X, y, groups, n_folds=5):\n    \"\"\"\n    Trains a ranking model with cross-validation\n    \n    Args:\n        X: Feature matrix\n        y: Target (1 for selected flight, 0 otherwise)\n        groups: ranker_id for group-wise splitting\n        n_folds: Number of cross-validation folds\n    \n    Returns:\n        model: Trained ranking model\n        oof_preds: Out-of-fold predictions\n    \"\"\"\n    models = []\n    oof_preds = np.zeros(len(X))\n    group_kfold = GroupKFold(n_splits=n_folds)\n    \n    for fold, (train_idx, val_idx) in enumerate(group_kfold.split(X, y, groups)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        groups_train = groups.iloc[train_idx]\n        \n        # Train LightGBM Ranker\n        model = LGBMRanker(\n            objective=\"lambdarank\",\n            metric=\"ndcg\",\n            boosting_type=\"gbdt\",\n            n_estimators=500,\n            learning_rate=0.05,\n            num_leaves=31,\n            max_depth=6,\n            min_child_samples=20,\n            random_state=42\n        )\n        \n        model.fit(\n            X_train, y_train,\n            group=groups_train.value_counts().sort_index().values,\n            eval_set=[(X_val, y_val)],\n            eval_group=[groups.iloc[val_idx].value_counts().sort_index().values],\n            early_stopping_rounds=50,\n            verbose=50\n        )\n        \n        # Store predictions and model\n        oof_preds[val_idx] = model.predict(X_val)\n        models.append(model)\n    \n    return models, oof_preds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. Converting Scores to Competition Ranks**","metadata":{}},{"cell_type":"code","source":"def scores_to_ranks(pred_scores, ranker_ids):\n    \"\"\"\n    Converts model scores to competition-required ranks\n    \n    Args:\n        pred_scores: Array of model predictions (higher = better)\n        ranker_ids: Corresponding ranker_id for each prediction\n    \n    Returns:\n        ranks: Array of ranks (1 = best) per flight option\n    \"\"\"\n    ranks = np.zeros_like(pred_scores)\n    for ranker_id in np.unique(ranker_ids):\n        mask = ranker_ids == ranker_id\n        group_scores = pred_scores[mask]\n        # argsort twice converts scores to ranks (1=best)\n        group_ranks = np.argsort(np.argsort(-group_scores)) + 1\n        ranks[mask] = group_ranks\n    return ranks.astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**4. Complete Workflow Example**","metadata":{}},{"cell_type":"code","source":"# Load data\ntrain = pd.read_csv(\"/kaggle/input/flightrank-2025/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/flightrank-2025/test.csv\")\n\n# Feature engineering (example features)\ndef create_features(df):\n    # Add your feature engineering here\n    df['price_per_mile'] = df['price'] / (df['distance'] + 1e-6)\n    return df\n\ntrain = create_features(train)\ntest = create_features(test)\n\n# Prepare for ranking model\nX_train = train.drop(['selected', 'ranker_id', 'Id'], axis=1)\ny_train = train['selected']\ngroups_train = train['ranker_id']\n\n# Train model\nmodels, oof_preds = train_ranker(X_train, y_train, groups_train)\n\n# Validate on training data (pseudo-test)\ntrain['pred_rank'] = scores_to_ranks(oof_preds, groups_train)\ntrain_hitrate = calculate_hitrate_at_3(train, train)\nprint(f\"OOF HitRate@3: {train_hitrate:.4f}\")\n\n# Prepare test predictions\nX_test = test.drop(['ranker_id', 'Id'], axis=1)\ntest_preds = np.mean([model.predict(X_test) for model in models], axis=0)\ntest['selected'] = scores_to_ranks(test_preds, test['ranker_id'])\n\n# Create submission\nsubmission = test[['Id', 'ranker_id', 'selected']]\nsubmission.to_csv(\"submission.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**5. Key Optimization Strategies**\n\n**1.Group-Aware Validation:**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GroupShuffleSplit\n\ngss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\ntrain_idx, val_idx = next(gss.split(X_train, y_train, groups_train))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Feature Importance Analysis:**","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': models[0].feature_importances_\n}).sort_values('importance', ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. Hyperparameter Tuning:**","metadata":{}},{"cell_type":"code","source":"param_grid = {\n    'num_leaves': [31, 63],\n    'learning_rate': [0.01, 0.05],\n    'n_estimators': [300, 500]\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**6. Final Submission**","metadata":{}},{"cell_type":"code","source":"def validate_submission(sub_df, test_df):\n    # Check row count matches\n    assert len(sub_df) == len(test_df)\n    \n    # Check rank validity per group\n    for ranker_id, group in sub_df.groupby('ranker_id'):\n        ranks = group['selected'].values\n        assert sorted(ranks) == list(range(1, len(ranks)+1))\n    \n    print(\"✅ Submission is valid!\")\n\nvalidate_submission(submission, test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"----------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"**Advanced Machine Learning Pipeline**\n\n**1. Feature Engineering Enhancements**","metadata":{}},{"cell_type":"code","source":"def enhanced_feature_engineering(interactions, users, flights):\n    \"\"\"Create advanced features for flight recommendations\"\"\"\n    # Merge datasets\n    data = interactions.merge(users, on='user_id', how='left')\n    data = data.merge(flights, on='flight_id', how='left')\n    \n    # Convert timestamps\n    data['interaction_time'] = pd.to_datetime(data['interaction_time'])\n    data['departure_time'] = pd.to_datetime(data['departure_time'])\n    \n    # Time-based features\n    data['days_to_departure'] = (data['departure_time'] - data['interaction_time']).dt.days\n    data['booking_lead_time'] = data['days_to_departure']\n    data['is_last_minute'] = (data['days_to_departure'] <= 3).astype(int)\n    data['is_advanced_booking'] = (data['days_to_departure'] > 21).astype(int)\n    \n    # User history features\n    user_history = data.groupby('user_id').agg({\n        'flight_id': 'count',\n        'price': ['mean', 'std'],\n        'airline': lambda x: x.mode()[0]\n    })\n    user_history.columns = ['user_flight_count', 'avg_price', 'price_std', 'preferred_airline']\n    data = data.merge(user_history, on='user_id', how='left')\n    \n    # Flight popularity features\n    flight_popularity = data['flight_id'].value_counts().reset_index()\n    flight_popularity.columns = ['flight_id', 'flight_popularity']\n    data = data.merge(flight_popularity, on='flight_id', how='left')\n    \n    # User-Flight affinity features\n    user_airline_counts = data.groupby(['user_id', 'airline']).size().unstack(fill_value=0)\n    data = data.merge(user_airline_counts, on='user_id', how='left')\n    \n    return data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Advanced Model Training with Hyperparameter Tuning**","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom lightgbm import LGBMRanker\n\ndef objective(trial, X_train, y_train, groups):\n    \"\"\"Optuna objective function for hyperparameter tuning\"\"\"\n    params = {\n        'objective': 'lambdarank',\n        'metric': 'ndcg',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True)\n    }\n    \n    model = LGBMRanker(**params)\n    model.fit(\n        X_train, \n        y_train,\n        group=groups,\n        eval_set=[(X_val, y_val)],\n        eval_group=[val_groups],\n        early_stopping_rounds=50,\n        verbose=False\n    )\n    \n    return model.best_score_['valid_0']['ndcg']\n\ndef train_best_model(X_train, y_train, groups):\n    \"\"\"Train model with optimal hyperparameters\"\"\"\n    study = optuna.create_study(direction='maximize')\n    study.optimize(lambda trial: objective(trial, X_train, y_train, groups), n_trials=50)\n    \n    best_params = study.best_params\n    best_params.update({\n        'objective': 'lambdarank',\n        'metric': 'ndcg',\n        'verbosity': 1\n    })\n    \n    best_model = LGBMRanker(**best_params)\n    best_model.fit(X_train, y_train, group=groups)\n    \n    return best_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. Ensemble Approach**","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator\nfrom sklearn.ensemble import VotingClassifier\n\nclass RankEnsemble(BaseEstimator):\n    \"\"\"Ensemble of ranking models\"\"\"\n    def __init__(self, models):\n        self.models = models\n        \n    def fit(self, X, y, groups):\n        for model in self.models:\n            model.fit(X, y, group=groups)\n        return self\n    \n    def predict(self, X):\n        predictions = np.zeros((X.shape[0], len(self.models)))\n        for i, model in enumerate(self.models):\n            predictions[:, i] = model.predict(X)\n        return np.mean(predictions, axis=1)\n\ndef create_ensemble():\n    \"\"\"Create ensemble of diverse ranking models\"\"\"\n    model1 = LGBMRanker(\n        objective='lambdarank',\n        metric='ndcg',\n        num_leaves=31,\n        learning_rate=0.05\n    )\n    \n    model2 = LGBMRanker(\n        objective='lambdarank',\n        metric='ndcg',\n        boosting_type='dart',\n        num_leaves=63,\n        learning_rate=0.01\n    )\n    \n    return RankEnsemble([model1, model2])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Complete Training Pipeline**","metadata":{}},{"cell_type":"code","source":"def full_pipeline():\n    # Load and preprocess data\n    data = enhanced_feature_engineering(interactions, users, flights)\n    \n    # Prepare features and target\n    X = data.drop(['user_id', 'flight_id', 'interaction_time', 'departure_time', 'target'], axis=1)\n    y = data['target']\n    groups = data.groupby('user_id').size().values\n    \n    # Time-based cross validation\n    best_ndcg = 0\n    best_model = None\n    \n    for train_idx, val_idx in time_based_cv(data):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        train_groups = groups[train_idx]\n        val_groups = groups[val_idx]\n        \n        # Train model with hyperparameter tuning\n        model = train_best_model(X_train, y_train, train_groups)\n        \n        # Evaluate\n        current_ndcg = evaluate_model(model, X_val, y_val)\n        if current_ndcg > best_ndcg:\n            best_ndcg = current_ndcg\n            best_model = model\n    \n    # Train final model on all data\n    final_model = train_best_model(X, y, groups)\n    \n    return final_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Submission Generation**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom lightgbm import LGBMRanker\nfrom sklearn.model_selection import GroupKFold\n\ndef load_data():\n    \"\"\"Load and validate competition data\"\"\"\n    try:\n        train = pd.read_csv('/kaggle/input/flightrank-2025-aeroclub-recsys-cup/train.csv')\n        test = pd.read_csv('/kaggle/input/flightrank-2025-aeroclub-recsys-cup/test.csv')\n        \n        # Basic validation\n        assert not train.empty, \"Train data is empty\"\n        assert not test.empty, \"Test data is empty\"\n        assert all(col in train.columns for col in ['Id', 'ranker_id', 'selected'])\n        assert all(col in test.columns for col in ['Id', 'ranker_id'])\n        \n        print(\"✅ Data loaded successfully\")\n        print(f\"Train samples: {len(train):,}, Test samples: {len(test):,}\")\n        print(f\"Unique search sessions - Train: {train['ranker_id'].nunique():,}, Test: {test['ranker_id'].nunique():,}\")\n        return train, test\n    \n    except Exception as e:\n        print(f\"❌ Error loading data: {str(e)}\")\n        return None, None\n\ndef create_features(df):\n    \"\"\"Create features for ranking model\"\"\"\n    # Basic features (customize based on available columns)\n    num_features = []\n    \n    if 'price' in df.columns:\n        df['price_norm'] = df.groupby('ranker_id')['price'].transform(lambda x: (x - x.mean()) / x.std())\n        num_features.append('price_norm')\n    \n    if 'duration' in df.columns:\n        df['duration_norm'] = df.groupby('ranker_id')['duration'].transform(lambda x: (x - x.mean()) / x.std())\n        num_features.append('duration_norm')\n    \n    if 'departure_time' in df.columns:\n        df['departure_hour'] = pd.to_datetime(df['departure_time']).dt.hour\n        df['departure_day'] = pd.to_datetime(df['departure_time']).dt.dayofweek\n        num_features.extend(['departure_hour', 'departure_day'])\n    \n    # Add more features as needed\n    print(f\"Generated {len(num_features)} numerical features\")\n    return df, num_features\n\ndef train_model(train_df, features):\n    \"\"\"Train LightGBM ranking model with cross-validation\"\"\"\n    X = train_df[features]\n    y = train_df['selected']\n    groups = train_df.groupby('ranker_id').size().values\n    \n    params = {\n        'objective': 'lambdarank',\n        'metric': 'ndcg',\n        'num_leaves': 63,\n        'learning_rate': 0.05,\n        'min_data_in_leaf': 50,\n        'feature_fraction': 0.8,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 1,\n        'random_state': 42\n    }\n    \n    model = LGBMRanker(**params)\n    \n    # GroupKFold validation\n    cv = GroupKFold(n_splits=3)\n    best_ndcg = 0\n    \n    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y, groups)):\n        print(f\"\\n🏁 Fold {fold + 1}\")\n        model.fit(\n            X.iloc[train_idx], y.iloc[train_idx],\n            group=groups[train_idx],\n            eval_set=[(X.iloc[val_idx], y.iloc[val_idx])],\n            eval_group=[groups[val_idx]],\n            eval_metric='ndcg',\n            early_stopping_rounds=50,\n            verbose=50\n        )\n        \n        # Track best fold\n        current_ndcg = model.best_score_['valid_0']['ndcg']\n        if current_ndcg > best_ndcg:\n            best_ndcg = current_ndcg\n            print(f\"🎯 New best NDCG: {best_ndcg:.4f}\")\n    \n    print(f\"\\n🔥 Best Validation NDCG: {best_ndcg:.4f}\")\n    return model\n\ndef generate_submission(model, test_df, features):\n    \"\"\"Generate properly formatted submission file\"\"\"\n    # Prepare test features\n    test_df = test_df.copy()\n    test_df, _ = create_features(test_df)\n    \n    # Predict scores (higher = better)\n    test_df['score'] = model.predict(test_df[features])\n    \n    # Convert scores to ranks (1 = best)\n    test_df['selected'] = test_df.groupby('ranker_id')['score'].rank(ascending=False, method='first')\n    test_df['selected'] = test_df['selected'].astype(int)\n    \n    # Create submission in correct format\n    submission = test_df[['Id', 'ranker_id', 'selected']].copy()\n    \n    # Validate before saving\n    if validate_submission(submission, test_df):\n        submission.to_csv('submission.csv', index=False)\n        print(\"\\n🎉 submission.csv created successfully!\")\n        print(\"Final submission preview:\")\n        display(submission.head())\n        \n        # Show some example rankings\n        example_session = submission['ranker_id'].iloc[0]\n        print(f\"\\nExample rankings for session '{example_session}':\")\n        display(submission[submission['ranker_id'] == example_session].sort_values('selected'))\n        \n        return submission\n    return None\n\ndef validate_submission(sub, test_df):\n    \"\"\"Validate submission meets all requirements\"\"\"\n    try:\n        print(\"\\n🔍 Validating submission...\")\n        \n        # Check 1: Maintains original row order\n        assert all(sub['Id'] == test_df['Id']), \"Row order changed from original test.csv\"\n        \n        # Check 2: Complete rankings for each search\n        ranker_groups = sub.groupby('ranker_id')\n        for name, group in ranker_groups:\n            expected_ranks = set(range(1, len(group) + 1))\n            actual_ranks = set(group['selected'])\n            assert actual_ranks == expected_ranks, f\"Invalid ranks for session {name}\"\n        \n        # Check 3: No duplicate ranks per search\n        assert not sub.duplicated(['ranker_id', 'selected']).any(), \"Duplicate ranks found\"\n        \n        # Check 4: All ranks ≥ 1\n        assert sub['selected'].min() >= 1, \"Ranks below 1 found\"\n        \n        print(\"✅ All validation checks passed!\")\n        return True\n    \n    except AssertionError as e:\n        print(f\"❌ Validation failed: {str(e)}\")\n        return False\n\n# Main Execution\nif __name__ == \"__main__\":\n    # Load data\n    print(\"📂 Loading data...\")\n    train_df, test_df = load_data()\n    \n    if train_df is not None and test_df is not None:\n        # Feature engineering\n        print(\"\\n🛠️ Creating features...\")\n        train_df, features = create_features(train_df)\n        print(\"Available features:\", features)\n        \n        # Train model\n        print(\"\\n🤖 Training ranking model...\")\n        model = train_model(train_df, features)\n        \n        # Generate submission\n        print(\"\\n💾 Creating submission file...\")\n        submission = generate_submission(model, test_df, features)\n        \n        if submission is not None:\n            print(f\"\\n✔ Successfully created submission file with {len(submission):,} rows\")\n            print(\"File saved as: submission.csv\")\n        else:\n            print(\"\\n❌ Failed to create valid submission file\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom lightgbm import LGBMRanker\nfrom sklearn.model_selection import GroupKFold\n\n# 1. Data Loading Function\ndef load_data():\n    \"\"\"Load and validate competition data\"\"\"\n    try:\n        train = pd.read_csv('/kaggle/input/flightrank-2025-aeroclub-recsys-cup/train.csv')\n        test = pd.read_csv('/kaggle/input/flightrank-2025-aeroclub-recsys-cup/test.csv')\n        \n        # Basic validation\n        assert not train.empty, \"Train data is empty\"\n        assert not test.empty, \"Test data is empty\"\n        assert all(col in train.columns for col in ['Id', 'ranker_id', 'selected'])\n        assert all(col in test.columns for col in ['Id', 'ranker_id'])\n        \n        print(\"✅ Data loaded successfully\")\n        print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n        return train, test\n    \n    except Exception as e:\n        print(f\"❌ Error loading data: {str(e)}\")\n        return None, None\n\n# 2. Feature Engineering Function\ndef create_features(df):\n    \"\"\"Create features for ranking model\"\"\"\n    data = df.copy()\n    \n    # Basic ranking features\n    if 'price' in data.columns:\n        data['price_rank'] = data.groupby('ranker_id')['price'].rank()\n    if 'duration' in data.columns:\n        data['duration_rank'] = data.groupby('ranker_id')['duration'].rank()\n    \n    # Normalized features\n    if 'price' in data.columns:\n        data['price_norm'] = data.groupby('ranker_id')['price'].transform(\n            lambda x: (x - x.mean()) / x.std())\n    \n    return data\n\n# 3. Model Training Function\ndef train_model(train_df):\n    \"\"\"Train ranking model with cross-validation\"\"\"\n    train_df = create_features(train_df)\n    features = [col for col in train_df.columns \n               if col not in ['Id', 'ranker_id', 'selected']]\n    \n    model = LGBMRanker(\n        objective=\"lambdarank\",\n        metric=\"ndcg\",\n        num_leaves=31,\n        learning_rate=0.05,\n        n_estimators=100\n    )\n    \n    model.fit(\n        train_df[features],\n        train_df['selected'],\n        group=train_df.groupby('ranker_id').size().values,\n        verbose=10\n    )\n    \n    return model, features\n\n# 4. Submission Generation Function\ndef create_submission(model, test_df, features):\n    \"\"\"Generate properly formatted submission file\"\"\"\n    test_df = create_features(test_df)\n    test_df['score'] = model.predict(test_df[features])\n    \n    # Convert scores to ranks (1 = best)\n    test_df['selected'] = test_df.groupby('ranker_id')['score'].rank(\n        ascending=False, method='first').astype(int)\n    \n    # Create final submission\n    submission = test_df[['Id', 'ranker_id', 'selected']].copy()\n    submission.to_csv('submission.csv', index=False)\n    \n    print(\"\\nFirst 5 rows of submission:\")\n    print(submission.head())\n    return submission\n\n# 5. Main Execution\nif __name__ == \"__main__\":\n    # Load data\n    train_df, test_df = load_data()\n    if train_df is None or test_df is None:\n        raise SystemExit(\"Failed to load data\")\n    \n    # Train model\n    model, features = train_model(train_df)\n    \n    # Create submission\n    submission = create_submission(model, test_df, features)\n    print(f\"\\n✅ submission.csv created successfully with {len(submission)} rows\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"**1. Enhanced Categorical Encoding**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\ndef enhanced_categorical_encoding(train_df, test_df, cat_cols):\n    \"\"\"\n    Enhanced categorical encoding with multiple strategies:\n    - Label encoding for high-cardinality features\n    - One-hot encoding for low-cardinality features\n    - Frequency encoding as fallback\n    \"\"\"\n    # Make copies to avoid SettingWithCopyWarning\n    train_df = train_df.copy()\n    test_df = test_df.copy()\n    \n    encoders = {}\n    \n    for col in cat_cols:\n        if col not in train_df.columns:\n            print(f\"Warning: Column {col} not found in training data\")\n            continue\n            \n        # Determine encoding strategy based on cardinality\n        n_unique = train_df[col].nunique()\n        \n        if n_unique > 20:  # High cardinality\n            # Label encoding with unseen values handling\n            le = LabelEncoder()\n            combined = pd.concat([train_df[col].astype(str), test_df[col].astype(str)])\n            le.fit(combined)\n            train_df[f\"{col}_le\"] = le.transform(train_df[col].astype(str))\n            test_df[f\"{col}_le\"] = le.transform(test_df[col].astype(str))\n            encoders[col] = {'type': 'label', 'encoder': le}\n            \n        elif 2 <= n_unique <= 20:  # Good for one-hot\n            # One-hot encoding with test set alignment\n            ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n            ohe.fit(train_df[[col]])\n            \n            # Get feature names\n            feature_names = [f\"{col}_{val}\" for val in ohe.categories_[0]]\n            \n            # Transform both sets\n            train_ohe = ohe.transform(train_df[[col]])\n            test_ohe = ohe.transform(test_df[[col]])\n            \n            # Create DataFrames for the encoded features\n            train_ohe_df = pd.DataFrame(train_ohe, columns=feature_names, index=train_df.index)\n            test_ohe_df = pd.DataFrame(test_ohe, columns=feature_names, index=test_df.index)\n            \n            # Concatenate with original DataFrames\n            train_df = pd.concat([train_df, train_ohe_df], axis=1)\n            test_df = pd.concat([test_df, test_ohe_df], axis=1)\n            encoders[col] = {'type': 'onehot', 'encoder': ohe}\n            \n        else:  # Very low cardinality or binary\n            # Simple label encoding\n            le = LabelEncoder()\n            le.fit(train_df[col].astype(str))\n            train_df[f\"{col}_le\"] = le.transform(train_df[col].astype(str))\n            test_df[f\"{col}_le\"] = le.transform(test_df[col].astype(str))\n            encoders[col] = {'type': 'label', 'encoder': le}\n    \n    return train_df, test_df, encoders\n\n# Define categorical columns (expand as needed)\ncat_cols = [\n    \"profileId\", \n    \"nationality\", \n    \"companyID\", \n    \"searchRoute\",\n    \"legs0_segments0_marketingCarrier_code\",\n    \"legs0_segments0_cabinClass\",\n    \"frequentFlyer\",\n    \"isVip\",\n    \"pricingInfo_isAccessTP\"\n]\n\n# Apply enhanced encoding\ntrain, test, categorical_encoders = enhanced_categorical_encoding(train, test, cat_cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Numerical Feature Scaling**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.pipeline import Pipeline\n\ndef scale_numerical_features(train_df, test_df, num_cols):\n    \"\"\"\n    Scale numerical features with robust scaling for outliers\n    Returns:\n    - Scaled DataFrames\n    - Fitted scaler object\n    - List of all numerical features (including generated ones)\n    \"\"\"\n    # Make copies to avoid SettingWithCopyWarning\n    train_df = train_df.copy()\n    test_df = test_df.copy()\n    \n    # Identify all numerical columns (including generated features)\n    all_num_cols = [\n        col for col in train_df.columns \n        if (train_df[col].dtype in ['int64', 'float64']) and \n        (col not in ['selected', 'Id', 'ranker_id'])\n    ]\n    \n    # If specific columns are provided, use those\n    final_num_cols = num_cols if num_cols else all_num_cols\n    \n    # Create scaling pipeline\n    scaler = Pipeline([\n        ('robust', RobustScaler()),  # Handles outliers\n        ('standard', StandardScaler())  # Standardizes to mean=0, std=1\n    ])\n    \n    # Fit on training data\n    scaler.fit(train_df[final_num_cols])\n    \n    # Transform both sets\n    train_scaled = scaler.transform(train_df[final_num_cols])\n    test_scaled = scaler.transform(test_df[final_num_cols])\n    \n    # Create DataFrames with scaled values\n    scaled_cols = [f\"{col}_scaled\" for col in final_num_cols]\n    train_scaled_df = pd.DataFrame(train_scaled, columns=scaled_cols, index=train_df.index)\n    test_scaled_df = pd.DataFrame(test_scaled, columns=scaled_cols, index=test_df.index)\n    \n    # Concatenate with original DataFrames\n    train_df = pd.concat([train_df, train_scaled_df], axis=1)\n    test_df = pd.concat([test_df, test_scaled_df], axis=1)\n    \n    return train_df, test_df, scaler, scaled_cols\n\n# Define numerical columns (or leave empty to auto-detect)\nnum_cols = [\n    \"totalPrice\",\n    \"taxes\",\n    \"duration_leg0\",\n    \"duration_leg1\",\n    \"price_per_hour\",\n    \"seatsAvailable_leg0\",\n    \"baggage_leg0\",\n    \"price_rank_in_group\",\n    \"duration_rank_in_group\",\n    \"seats_rank_in_group\"\n]\n\n# Apply scaling\ntrain, test, numerical_scaler, scaled_features = scale_numerical_features(train, test, num_cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. Combined Feature Preparation Pipeline**","metadata":{}},{"cell_type":"code","source":"def prepare_features(train_df, test_df):\n    \"\"\"\n    Complete feature preparation pipeline:\n    1. Basic feature engineering\n    2. Categorical encoding\n    3. Numerical scaling\n    \"\"\"\n    # Step 1: Basic feature engineering\n    train_df = basic_time_and_numeric_features(train_df)\n    test_df = basic_time_and_numeric_features(test_df)\n    \n    train_df = user_and_policy_features(train_df)\n    test_df = user_and_policy_features(test_df)\n    \n    train_df = group_relative_features(train_df)\n    test_df = group_relative_features(test_df)\n    \n    # Step 2: Categorical encoding\n    cat_cols = [\n        \"profileId\", \"nationality\", \"companyID\", \"searchRoute\",\n        \"legs0_segments0_marketingCarrier_code\", \"legs0_segments0_cabinClass\",\n        \"frequentFlyer\", \"isVip\", \"pricingInfo_isAccessTP\"\n    ]\n    train_df, test_df, cat_encoders = enhanced_categorical_encoding(train_df, test_df, cat_cols)\n    \n    # Step 3: Numerical scaling\n    num_cols = [\n        \"totalPrice\", \"taxes\", \"duration_leg0\", \"duration_leg1\",\n        \"price_per_hour\", \"seatsAvailable_leg0\", \"baggage_leg0\",\n        \"price_rank_in_group\", \"duration_rank_in_group\", \"seats_rank_in_group\"\n    ]\n    train_df, test_df, num_scaler, scaled_cols = scale_numerical_features(train_df, test_df, num_cols)\n    \n    # Return prepared data and encoders\n    return {\n        'train': train_df,\n        'test': test_df,\n        'categorical_encoders': cat_encoders,\n        'numerical_scaler': num_scaler,\n        'scaled_features': scaled_cols\n    }\n\n# Execute complete pipeline\nprepared_data = prepare_features(train, test)\ntrain_prepared = prepared_data['train']\ntest_prepared = prepared_data['test']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**4. Evaluation and Submission**","metadata":{}},{"cell_type":"code","source":"class FlightEvaluator:\n    @staticmethod\n    def hit_rate_at_k(y_true, y_pred, groups, k=3):\n        \"\"\"Calculate HitRate@k metric\"\"\"\n        results = []\n        group_sizes = []\n        \n        for group in groups.unique():\n            group_mask = (groups == group)\n            y_true_group = y_true[group_mask]\n            y_pred_group = y_pred[group_mask]\n            \n            # Only consider groups with >10 options as per competition rules\n            if len(y_true_group) > 10:\n                top_k = y_pred_group.argsort()[-k:][::-1]\n                hit = y_true_group.iloc[top_k].sum() > 0\n                results.append(hit)\n                group_sizes.append(len(y_true_group))\n                \n        return np.mean(results), np.sum(group_sizes)\n    \n    @staticmethod\n    def create_submission(test_df, preds, ranker_id_col='ranker_id'):\n        \"\"\"Create competition submission file\"\"\"\n        submission = test_df[['Id', ranker_id_col]].copy()\n        \n        for ranker_id in tqdm(submission[ranker_id_col].unique(), desc='Creating ranks'):\n            mask = submission[ranker_id_col] == ranker_id\n            submission.loc[mask, 'selected'] = (-preds[mask]).argsort().argsort() + 1\n            \n        # Validate submission\n        FlightEvaluator._validate_submission(submission, ranker_id_col)\n        \n        return submission\n    \n    @staticmethod\n    def _validate_submission(submission, ranker_id_col):\n        \"\"\"Validate submission meets competition requirements\"\"\"\n        errors = []\n        \n        # Check for complete rankings per group\n        for ranker_id in submission[ranker_id_col].unique():\n            group = submission[submission[ranker_id_col] == ranker_id]\n            ranks = group['selected']\n            \n            # Check for valid permutation\n            if not np.array_equal(np.sort(ranks), np.arange(1, len(ranks)+1)):\n                errors.append(f\"Invalid ranks for {ranker_id}\")\n                \n        if errors:\n            raise ValueError(f\"Submission validation failed: {errors[:3]}...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**5. Full Pipeline Execution**","metadata":{}},{"cell_type":"code","source":"def main():\n    # Initialize and load data\n    data_processor = FlightDataProcessor('/kaggle/input/aeroclub-recsys-2025')\n    data_processor.load_base_data()\n    \n    # Process JSON features (sample for demonstration)\n    try:\n        data_processor.process_json_features(sample_size=1000)\n        data_processor.merge_features()\n    except Exception as e:\n        print(f\"JSON processing skipped: {str(e)}\")\n    \n    # Feature engineering\n    feature_engineer = FeatureEngineer()\n    train = feature_engineer.create_time_features(data_processor.train)\n    train = feature_engineer.create_flight_features(train)\n    train = feature_engineer.create_price_features(train)\n    \n    # Prepare modeling data\n    X = train.drop(columns=['selected', 'Id', 'ranker_id'])\n    y = train['selected']\n    groups = train['ranker_id']\n    \n    # Train models\n    ranker = FlightRanker()\n    ranker.train(X, y, groups)\n    \n    # Evaluate\n    for model_name, preds in ranker.oof_preds.items():\n        hr, n_groups = FlightEvaluator.hit_rate_at_k(y, preds, groups)\n        print(f\"{model_name} OOF HitRate@3: {hr:.4f} (evaluated on {n_groups} groups)\")\n    \n    # Create ensemble predictions\n    test = feature_engineer.create_time_features(data_processor.test)\n    test = feature_engineer.create_flight_features(test)\n    test = feature_engineer.create_price_features(test)\n    X_test = test.drop(columns=['Id', 'ranker_id'])\n    \n    ensemble_preds = ranker.ensemble_predict(X_test)\n    submission = FlightEvaluator.create_submission(test, ensemble_preds)\n    \n    # Save submission\n    submission.to_csv('submission.csv', index=False)\n    print(\"Submission file saved successfully.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}